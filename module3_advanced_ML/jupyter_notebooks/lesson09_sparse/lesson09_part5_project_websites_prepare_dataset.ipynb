{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ml_theme.png\">\n",
    "# Дополнительное профессиональное <br> образование НИУ ВШЭ\n",
    "#### Программа \"Практический анализ данных и машинное обучение\"\n",
    "<img src=\"../../img/faculty_logo.jpg\" height=\"240\" width=\"240\">\n",
    "## Автор материала: преподаватель Факультета Компьютерных Наук НИУ ВШЭ <br> Кашницкий Юрий\n",
    "</center>\n",
    "Материал распространяется на условиях лицензии <a href=\"https://opensource.org/licenses/MS-RL\">Ms-RL</a>. Можно использовать в любых целях, кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Занятие 9. Разреженные данные, онлайн-обучение</center>\n",
    "\n",
    "## <center>Часть 5. Идентификация пользователей по посещенным веб-страницам</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "\n",
    "Будем решать задачу идентификации пользователя по его поведению в сети Интернет. Это сложная и интересная задача на стыке анализа данных и поведенческой психологии. В качестве примера, компания Яндекс решает задачу идентификации взломщика почтового ящика по его поведению. В двух словах, взломщик будет себя вести не так, как владелец ящика: он может не удалять сообщения сразу по прочтении, как это делал хозяин, он будет по-другому ставить флажки сообщениям и даже по-своему двигать мышкой. Тогда такого злоумышленника можно идентифицировать и \"выкинуть\" из почтового ящика, предложив хозяину войти по SMS-коду. Этот пилотный проект описан в [статье](https://habrahabr.ru/company/yandex/blog/230583/) на Хабрахабре. Похожие вещи делаются, например, в Google Analytics и описываются в научных статьях, найти можно многое по фразам \"Traversal Pattern Mining\" и \"Sequential Pattern Mining\".\n",
    "\n",
    "\n",
    "Мы будем решать похожую задачу: по последовательности из нескольких веб-сайтов, посещенных подряд один и тем же человеком, мы будем идентифицировать этого человека. Идея такая: пользователи Интернета по-разному переходят по ссылкам, и это может помогать их идентифицировать (кто-то сначала в почту, потом про футбол почитать, затем новости, контакт, потом наконец – работать, кто-то – сразу работать).\n",
    "\n",
    "Будем использовать данные из [статьи](http://ceur-ws.org/Vol-1058/) \"A Tool for Classification of Sequential Data\". И хотя мы не можем рекомендовать эту статью (описанные методы делеки от state-of-the-art, лучше обращаться к [книге](http://www.charuaggarwal.net/freqbook.pdf) \"Frequent Pattern Mining\" и последним статьям с ICDM), но данные там собраны аккуратно и представляют интерес.\n",
    "\n",
    "Данные собраны с прокси-серверов Университета Блеза Паскаля и имеют очень простой вид: \n",
    "<center>ID пользователя, timestamp, посещенный веб-сайт</center>\n",
    "\n",
    "Скачать исходные данные можно по [ссылке](http://fc.isima.fr/~kahngi/cez13.zip) в статье, там же описание.\n",
    "Для этого задания хватит данных не по всем 3000 пользователям, а по 10 и 150. [Ссылка](https://yadi.sk/d/_HK76ZDo32AvNZ) на архив *capstone_websites_data.zip* (~7.1 Mb, в развернутом виде ~ 65 Mb). \n",
    "\n",
    "В финальном проекте уже придется столкнуться с тем, что не все операции можно выполнить за разумное время (скажем, перебрать с кросс-валидацией 100 комбинаций параметров случайного леса на этих данных Вы вряд ли сможете), поэтому мы будем использовать параллельно 2 выборки - по 10 пользователям и по 150. Для 10 пользователей будем писать и отлаживать код, для 150 – будет рабочая версия. \n",
    "\n",
    "Данные устроены следующем образом:\n",
    "\n",
    " - В каталоге 10users лежат 10 csv-файлов с названием вида \"user[USER_ID].csv\", где [USER_ID] - ID пользователя;\n",
    " - Аналогично для каталога 150users - там 150 файлов;\n",
    " - В 3users_toy – игрушечный пример из 3 файлов, это для отладки кода предобработки, который Вы далее напишете.\n",
    " \n",
    "В этой части задания мы подготовим данные для дальнейшего анализа и обучения. \n",
    "\n",
    "\n",
    "# <center>Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций 1 и 2 недели курса \"Математика и Python для анализа данных\":**\n",
    "   - [Циклы, функции, генераторы, list comprehension](https://www.coursera.org/learn/mathematics-and-python/lecture/Kd7dL/tsikly-funktsii-ghienieratory-list-comprehension)\n",
    "   - [Чтение данных из файлов](https://www.coursera.org/learn/mathematics-and-python/lecture/8Xvwp/chtieniie-dannykh-iz-failov)\n",
    "   - [Запись файлов, изменение файлов](https://www.coursera.org/learn/mathematics-and-python/lecture/vde7k/zapis-failov-izmienieniie-failov)\n",
    "   - [Pandas.DataFrame](https://www.coursera.org/learn/mathematics-and-python/lecture/rcjAW/pandas-data-frame)\n",
    "   - [Pandas. Индексация и селекция](https://www.coursera.org/learn/mathematics-and-python/lecture/lsXAR/pandas-indieksatsiia-i-sieliektsiia)\n",
    "   \n",
    "**Кроме того, в задании будут использоваться бибилиотеки Python [glob](https://docs.python.org/3/library/glob.html), [pickle](https://docs.python.org/2/library/pickle.html) и класс [csr_matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html) из scipy.sparse.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, для лучшей воспроизводимости результатов приведем список версий основных используемых в проекте библиотек: NumPy, SciPy, Pandas, Matplotlib, Statsmodels и Scikit-learn. Для этого воспользуемся расширением [watermark](https://github.com/rasbt/watermark). Автор задания использует Anaconda 3 версии 4.2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.5.2\n",
      "IPython 5.1.0\n",
      "\n",
      "numpy 1.11.2\n",
      "scipy 0.18.1\n",
      "pandas 0.19.1\n",
      "matplotlib 1.5.3\n",
      "statsmodels 0.8.0rc1\n",
      "scikit-learn 0.18\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)\n",
      "system     : Darwin\n",
      "release    : 16.1.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : 588073fb477931d021194d65882434d1ce6ff7ce\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,statsmodels,scikit-learn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример файла с данными о посещенных пользователем (номер 31) веб-страницах.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# поменяйте путь \n",
    "user31_data = pd.read_csv('/Users/yorko/Яндекс.Диск/ML/data/capstone_websites_data/10users/user0031.csv', header=None,\n",
    "                  names=['id', 'timestamp', 'site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:17:10</td>\n",
       "      <td>api.dailymotion.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:21:43</td>\n",
       "      <td>b12.myspace.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:21:46</td>\n",
       "      <td>b12.myspace.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:22:14</td>\n",
       "      <td>www.facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:37:30</td>\n",
       "      <td>pixel2368.everesttech.net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            timestamp                       site\n",
       "0  31  2013-11-15T08:17:10        api.dailymotion.com\n",
       "1  31  2013-11-15T08:21:43            b12.myspace.com\n",
       "2  31  2013-11-15T08:21:46            b12.myspace.com\n",
       "3  31  2013-11-15T08:22:14           www.facebook.com\n",
       "4  31  2013-11-15T08:37:30  pixel2368.everesttech.net"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user31_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Поставим задачу классификации: идентифицировать пользователя по сессии из 10 подряд посещенных сайтов. Объектом в этой задаче будет сессия из 10 сайтов, последовательно посещенных одним и тем же пользователем, признаками – индексы этих 10 сайтов (чуть позже здесь появится \"мешок\" сайтов, подход Bag of Words). Целевым классом будет id пользователя.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Пример для иллюстрации</center>\n",
    "**Пусть пользователя всего 2, длина сессии – 2 сайта.**\n",
    "\n",
    "<center>user1.csv</center>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">id</th>\n",
    "    <th class=\"tg-031e\">timestamp</th>\n",
    "    <th class=\"tg-031e\">site</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">1</td>\n",
    "    <td class=\"tg-031e\">00:00:01</td>\n",
    "    <td class=\"tg-031e\">vk.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">1</td>\n",
    "    <td class=\"tg-yw4l\">00:00:11</td>\n",
    "    <td class=\"tg-yw4l\">google.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">1</td>\n",
    "    <td class=\"tg-031e\">00:00:16</td>\n",
    "    <td class=\"tg-031e\">vk.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">1</td>\n",
    "    <td class=\"tg-031e\">00:00:20</td>\n",
    "    <td class=\"tg-031e\">yandex.ru</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<center>user2.csv</center>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">id</th>\n",
    "    <th class=\"tg-031e\">timestamp</th>\n",
    "    <th class=\"tg-031e\">site</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">2</td>\n",
    "    <td class=\"tg-031e\">00:00:02</td>\n",
    "    <td class=\"tg-031e\">yandex.ru</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">2</td>\n",
    "    <td class=\"tg-yw4l\">00:00:14</td>\n",
    "    <td class=\"tg-yw4l\">google.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">2</td>\n",
    "    <td class=\"tg-031e\">00:00:17</td>\n",
    "    <td class=\"tg-031e\">facebook.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">2</td>\n",
    "    <td class=\"tg-031e\">00:00:25</td>\n",
    "    <td class=\"tg-031e\">yandex.ru</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Идем по 1 файлу, нумеруем сайты подряд: vk.com – 1, google.com – 2 и т.д. Далее по второму файлу. \n",
    "\n",
    "Отображение сайтов в их индесы должно получиться таким:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">site</th>\n",
    "    <th class=\"tg-yw4l\">site_id</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">vk.com</td>\n",
    "    <td class=\"tg-yw4l\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">google.com</td>\n",
    "    <td class=\"tg-yw4l\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">yandex.ru</td>\n",
    "    <td class=\"tg-yw4l\">3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">facebook.com</td>\n",
    "    <td class=\"tg-yw4l\">4</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Тогда обучающая выборка будет такой:\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-s6z2{text-align:center}\n",
    ".tg .tg-baqh{text-align:center;vertical-align:top}\n",
    ".tg .tg-hgcj{font-weight:bold;text-align:center}\n",
    ".tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-hgcj\">session_id</th>\n",
    "    <th class=\"tg-hgcj\">site1</th>\n",
    "    <th class=\"tg-hgcj\">site2</th>\n",
    "    <th class=\"tg-amwm\">target</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-baqh\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-baqh\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-baqh\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">4</td>\n",
    "    <td class=\"tg-s6z2\">4</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-baqh\">2</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Здесь 1 объект – это сессия из 2 посещенных сайтов 1-ым пользователем (target=1). Это сайты vk.com и google.com (номер 1 и 2). И так далее, всего 4 сессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 \n",
    "Реализуйте функцию *prepare_train_set*, которая принимает на вход путь к каталогу с csv-файлами и параметр *session_length* - длину сессии, а возвращает 2 объекта:\n",
    "- матрицу (двухмерный NumPy array), в которой строки соответствуют сессиям из *session_length* сайтов, *session_length* столбцов - индексам этих *session_length* сайтов и последний столбец - ID пользователя. \n",
    "- частотный словарь сайтов вида {'site_string': [site_id, site_freq]}, например для недавнего игрушечного примера это будет {'vk.com': [1, 2], 'google.com': [2, 2], 'yandex.ru': [3, 3], 'facebook.com': [4, 1]}\n",
    "\n",
    "Детали:\n",
    "- Используйте glob (или аналоги) для обхода файлов в каталоге\n",
    "- Создайте частотный словарь уникальных сайтов (вида {'site_string': (site_id, site_freq)}) и заполняйте его по ходу чтения файлов. Начинайте с 1, то есть первый попавшийся сайт кодируйте единицей\n",
    "- Не делайте entity recognition, считайте *google.com*, *http://www.google.com* и *www.google.com* разными сайтами\n",
    "- Скорее всего в файле число записей не кратно числу *session_length*. Тогда последняя сессия будет короче. Остаток заполняйте нулями. То есть если в файле 24 записи и сессии длины 10, то 3 сессия будет состоять из 4 сайтов, и ей мы сопоставим вектор [*site1_id*, *site2_id*, *site3_id*, *site4_id*, 0, 0, 0, 0, 0, 0, *user_id*] \n",
    "- 150 файлов из *capstone_websites_data/150users/* на моем ноуте обработались за 30 секунд, но многое, конечно, зависит от эффективности реализации функции *prepare_train_set* и от используемого железа (Если запускать эту функцию для исходных данных по 3000 пользователям, то уже неплохо сделать профилирование, чтоб оптимизировать выполнение функции *prepare_train_set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_train_set(csv_files_mask, session_length=10):\n",
    "    from time import time\n",
    "    init_time = time()\n",
    "    site_id = 1\n",
    "    num_sessions_added = 0\n",
    "    site_id_and_freq = {}\n",
    "    X = np.array([0] * (session_length + 1)).reshape([1, session_length + 1])\n",
    "    for file_id, file_name in enumerate(glob(csv_files_mask)):\n",
    "        print(file_id, end=' ')\n",
    "        data = pd.read_csv(file_name, header=None,\n",
    "                      names=['id', 'timestamp', 'site'])\n",
    "\n",
    "        user, sites = int(data['id'][0]), data['site']\n",
    "        \n",
    "        num_sessions_to_add = (sites.count() - 1) // session_length + 1\n",
    "        X = np.vstack([X, np.zeros([num_sessions_to_add,\n",
    "                                       session_length + 1])])\n",
    "        \n",
    "        for session_id in range(0, num_sessions_to_add):\n",
    "            for row_id in range(session_id * session_length, min(sites.count(),\n",
    "                                                              (session_id + 1) * session_length)):\n",
    "\n",
    "                site = sites.iloc[row_id]\n",
    "                if site not in site_id_and_freq:\n",
    "                    site_id_and_freq[site] = [site_id, 1]\n",
    "                    current_site_id = site_id\n",
    "                    site_id += 1\n",
    "                else:\n",
    "                    site_id_and_freq[site][1] += 1\n",
    "                    current_site_id = site_id_and_freq[site][0]\n",
    "                \n",
    "                j = row_id - session_id * session_length\n",
    "                \n",
    "                X[num_sessions_added + session_id + 1, j] = current_site_id\n",
    "                X[num_sessions_added + session_id + 1, session_length] = user\n",
    "\n",
    "        num_sessions_added += num_sessions_to_add\n",
    "    \n",
    "        \n",
    "    X = np.delete(X, 0, axis=0)\n",
    "    print(time() - init_time)\n",
    "    \n",
    "    return X, site_id_and_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примените полученную функцию к игрушечному примеру, убедитесь, что все работает как надо. В этом можно убедиться просто глазами, а также проверив 2 выражения assert ниже. Если все правильно, ничего не произойдет при выполнении этой клетки, если код prepare_train_set реализован неправильно – будет AssertionError.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 0.020897865295410156\n"
     ]
    }
   ],
   "source": [
    "# поменяйте путь\n",
    "train_data_toy, site_freq_3users = prepare_train_set('/Users/yorko/Яндекс.Диск/ML/data/capstone_websites_data/3users_toy/*', \n",
    "                                                     session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   2.,   2.,   3.,   2.,   4.,   5.,   6.,   7.,   8.,   1.],\n",
       "       [  1.,   4.,   4.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,   1.],\n",
       "       [  1.,   2.,   9.,   9.,   2.,   0.,   0.,   0.,   0.,   0.,   2.],\n",
       "       [ 10.,   4.,   2.,   4.,   2.,   4.,   4.,   6.,  11.,  10.,   3.],\n",
       "       [ 10.,   4.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vk.com', [1, 3]),\n",
       " ('oracle.com', [2, 8]),\n",
       " ('geo.mozilla.org', [3, 1]),\n",
       " ('google.com', [4, 9]),\n",
       " ('accounts.google.com', [5, 1]),\n",
       " ('mail.google.com', [6, 2]),\n",
       " ('apis.google.com', [7, 1]),\n",
       " ('plus.google.com', [8, 1]),\n",
       " ('football.kulichki.ru', [9, 2]),\n",
       " ('meduza.io', [10, 3]),\n",
       " ('yandex.ru', [11, 1])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(site_freq_3users.items(), key=lambda pair: pair[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert train_data_toy.sum() == 159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert site_freq_3users['meduza.io'] == [10, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Примените полученную функцию к данным по 10 пользователям, посмотрите, сколько уникальных сессий из 10 сайтов получится, запишите ответ в файл с помощью функции write_answer_to_file – это будет ответом к первому вопросу. Если ответ получился неправильный, значит где-то ошибка в функции prepare_train_set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 16.63930916786194\n"
     ]
    }
   ],
   "source": [
    "# поменяйте путь\n",
    "train_data_10users, site_freq_10users =\\\n",
    "    prepare_train_set('/Users/yorko/Яндекс.Диск/ML/data/capstone_websites_data/10users/*', \n",
    "                                                          session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14061"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_10users.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Сколько всего уникальных сайтов в выборке из 10 пользователей? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4913"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(site_freq_10users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Примените полученную функцию к данным по 150 пользователям, посмотрите, сколько уникальных сессий из 10 сайтов получится, запишите ответ в файл с помощью функции write_answer_to_file – это будет ответом к третьему вопросу. Если ответ получился неправильный, значит где-то ошибка в функции prepare_train_set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 145.9791350364685\n",
      "CPU times: user 2min 17s, sys: 2.17 s, total: 2min 19s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_150users, site_freq_150users = \\\n",
    "    prepare_train_set('/Users/yorko/Яндекс.Диск/ML/data/capstone_websites_data/150users/*', \n",
    "                                                         session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137019"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_150users.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Сколько всего уникальных сайтов в выборке из 150 пользователей? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27797"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(site_freq_150users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Выведите топ-10 самых популярных сайтов среди посещенных 150 пользователями. Запишите ответ в файл *answer1_5.txt* через пробел – все 10 сайтов на одной строке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top10_popular = sorted(site_freq_150users.items(), key=lambda key_val: -key_val[1][1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('www.google.fr', [1, 64785]),\n",
       " ('www.google.com', [3, 51320]),\n",
       " ('www.facebook.com', [41, 39002]),\n",
       " ('apis.google.com', [2, 29983]),\n",
       " ('s.youtube.com', [178, 29102]),\n",
       " ('clients1.google.com', [212, 25087]),\n",
       " ('mail.google.com', [28, 19072]),\n",
       " ('plus.google.com', [27, 18467]),\n",
       " ('safebrowsing-cache.google.com', [293, 17960]),\n",
       " ('www.youtube.com', [105, 16319])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для дальнейшего анализа превратим полученные матрицы опять в DataFrame и запишем в csv-файлы.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_10users = pd.DataFrame(train_data_10users, \n",
    "                        columns=['site' + str(i) for i in range(1,11)] + ['target'])\n",
    "train_df_150users = pd.DataFrame(train_data_150users, \n",
    "                        columns=['site' + str(i) for i in range(1,11)] + ['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним полученные DataFrame в csv-файлы, при этом учтем, что все признаки у нас целочисленные, поэтому запишем их в соответствующем формате: у DataFrame.to_csv аргумент float_format='%d'. Так на одних только символах разделителя и последующего нуля можно сэкономить почти 30% места."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df_10users.to_csv('capstone_websites_data/train_data_10users.csv', \n",
    "#                         index_label='session_id', float_format='%d')\n",
    "# train_df_150users.to_csv('capstone_websites_data/train_data_150users.csv', \n",
    "#                          index_label='session_id', float_format='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если так подумать, то полученные признаки site1, ..., site10 смысла не имеют как признаки в задаче классификации. А вот если воспользоваться идеей мешка слов из анализа текстов – это другое дело. Создадим новые матрицы, в которых строкам будут соответствовать сессии из 10 сайтов, а столбцам – индексы сайтов. На пересечении строки $i$ и столбца $j$ будет стоять число $n_{ij}$ – cколько раз сайт $j$ встретился в сессии номер $i$. Делать это будем с помощью разреженных матриц Scipy – [csr_matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html). Прочитайте документацию, разберитесь, как использовать разреженные матрицы и создайте такие матрицы для наших данных. Сначала проверьте на игрушечном примере, затем примените для 10 и 150 пользователей. \n",
    "\n",
    "Обратите внимание, что в коротких сессиях, меньше 10 сайтов, у нас остались нули, так что первый признак (сколько раз попался 0) по смыслу отличен от остальных (сколько раз попался сайт с индексом $i$). Поэтому первый столбец разреженной матрицы надо будет удалить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_toy, y_toy = train_data_toy[:, :-1], train_data_toy[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sparse_toy = csr_matrix(([1] * X_toy.flatten().shape[0], X_toy.flatten(), \n",
    "                range(0, X_toy.flatten().shape[0] + 10, 10)))[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0],\n",
       "        [0, 2, 0, 4, 0, 1, 0, 0, 0, 2, 1],\n",
       "        [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sparse_toy.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_10users, y_10users = train_data_10users[:, :-1], train_data_10users[:, -1]\n",
    "X_150users, y_150users = train_data_150users[:, :-1], train_data_150users[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sparse_10users = csr_matrix(([1] * X_10users.flatten().shape[0], X_10users.flatten(), \n",
    "                range(0, X_10users.flatten().shape[0] + 10, 10)))[:, 1:]\n",
    "X_sparse_150users = csr_matrix(([1] * X_150users.flatten().shape[0], X_150users.flatten(), \n",
    "                range(0, X_150users.flatten().shape[0] + 10, 10)))[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраните эти разреженные матрицы с помощью [pickle](https://docs.python.org/2/library/pickle.html) (сериализация в Python), также сохраните вектора y_10users, y_150users – целевые значения (id пользователя)  в выборках из 10 и 150 пользователей. То что названия этих матриц начинаются с X и y, намекает на то, что на этих данных мы будем проверять первые модели классификации.\n",
    "Наконец, сохраните также и частотный словарь сайтов для 3, 10 и 150 пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# поменяйте путь\n",
    "path_to_save_pickle = '/Users/yorko/Яндекс.Диск/ML/data/capstone_websites_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_save_pickle, 'X_sparse_10users.pkl'), 'wb') as X10_pkl:\n",
    "    pickle.dump(X_sparse_10users, X10_pkl)\n",
    "with open(os.path.join(path_to_save_pickle, 'y_10users.pkl'), 'wb') as y10_pkl:\n",
    "    pickle.dump(y_10users, y10_pkl)\n",
    "with open(os.path.join(path_to_save_pickle, 'X_sparse_150users.pkl'), 'wb') as X150_pkl:\n",
    "    pickle.dump(X_sparse_150users, X150_pkl)\n",
    "with open(os.path.join(path_to_save_pickle, 'y_150users.pkl'), 'wb') as y150_pkl:\n",
    "    pickle.dump(y_150users, y150_pkl)\n",
    "with open(os.path.join(path_to_save_pickle, 'site_freq_3users.pkl'), 'wb') as site_freq_3users_pkl:\n",
    "    pickle.dump(site_freq_3users, site_freq_3users_pkl)\n",
    "with open(os.path.join(path_to_save_pickle, 'site_freq_10users.pkl'), 'wb') as site_freq_10users_pkl:\n",
    "    pickle.dump(site_freq_10users, site_freq_10users_pkl)\n",
    "with open(os.path.join(path_to_save_pickle, 'site_freq_150users.pkl'), 'wb') as site_freq_150users_pkl:\n",
    "    pickle.dump(site_freq_150users, site_freq_150users_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Убедитесь, что число столбцов в разреженных матрицах X_sparse_10users и X_sparse_150users равно ранее посчитанным числам уникальных сайтов для 10 и 150 пользователей соответственно. Важно убедиться, что потом мы все будем работать с одними и теми же данными.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert X_sparse_10users.shape[1] == len(site_freq_10users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert X_sparse_150users.shape[1] == len(site_freq_150users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
